#!/bin/bash
# CLI Examples for TRIDENT-Net Memory-Efficient Training

echo "TRIDENT-Net Memory-Efficient Training Examples"
echo "=============================================="

# Example 1: DeepSpeed ZeRO-2 Offload
echo -e "\n1. DeepSpeed ZeRO-2 Offload (Variant A)"
echo "Command:"
echo "deepspeed --num_gpus 1 -m trident.runtime.memory_efficient_cli \\"
echo "  --config tasks.yml \\"
echo "  --use-bf16 \\"
echo "  --checkpoint-every-layer \\"
echo "  --grad-accum-steps 8 \\"
echo "  --optimizer adamw8bit \\"
echo "  --zero-stage 2 \\"
echo "  --synthetic"

echo -e "\nAlternatively with explicit DeepSpeed config:"
echo "deepspeed --num_gpus 1 \\"
echo "  --deepspeed ds_config.json \\"
echo "  -m trident.runtime.memory_efficient_cli \\"
echo "  --config tasks.yml \\"
echo "  --synthetic"

# Example 2: HF Accelerate
echo -e "\n2. HF Accelerate Device Mapping (Variant B)"
echo "Command:"
echo "python -m trident.runtime.memory_efficient_cli \\"
echo "  --config tasks.yml \\"
echo "  --use-bf16 \\"
echo "  --checkpoint-every-layer \\"
echo "  --grad-accum-steps 8 \\"
echo "  --optimizer paged_adamw8bit \\"
echo "  --device-map auto \\"
echo "  --max-gpu-mem 39GiB \\"
echo "  --cpu-mem 70GiB \\"
echo "  --zero-stage 0 \\"
echo "  --synthetic"

# Example 3: QLoRA variant
echo -e "\n3. QLoRA + Memory Optimizations"
echo "Command:"
echo "python -m trident.runtime.memory_efficient_cli \\"
echo "  --config tasks.yml \\"
echo "  --use-bf16 \\"
echo "  --checkpoint-every-layer \\"
echo "  --grad-accum-steps 16 \\"
echo "  --optimizer paged_adamw8bit \\"
echo "  --qlora \\"
echo "  --device-map auto \\"
echo "  --max-gpu-mem 39GiB \\"
echo "  --synthetic"

# Example 4: Conservative settings
echo -e "\n4. Conservative Memory Settings"
echo "Command:"
echo "python -m trident.runtime.memory_efficient_cli \\"
echo "  --config tasks.yml \\"
echo "  --use-bf16 \\"
echo "  --checkpoint-every-layer \\"
echo "  --grad-accum-steps 16 \\"
echo "  --optimizer paged_adamw8bit \\"
echo "  --zero-stage 3 \\"
echo "  --batch-size 1 \\"
echo "  --synthetic"

# Example 5: Smoke test
echo -e "\n5. Smoke Test (validate memory usage)"
echo "Command:"
echo "python test_smoke.py"

# Example 6: Real data training
echo -e "\n6. Real Data Training (Example)"
echo "Command:"
echo "python -m trident.runtime.memory_efficient_cli \\"
echo "  --config tasks.yml \\"
echo "  --jsonl /path/to/data.jsonl \\"
echo "  --video-root /path/to/videos \\"
echo "  --use-bf16 \\"
echo "  --checkpoint-every-layer \\"
echo "  --grad-accum-steps 8 \\"
echo "  --optimizer adamw8bit \\"
echo "  --zero-stage 2 \\"
echo "  --batch-size 1"

echo -e "\nMemory Optimization Flags:"
echo "=========================="
echo "--use-bf16              : Enable BF16 mixed precision (recommended)"
echo "--checkpoint-every-layer: Enable activation checkpointing for heavy blocks"
echo "--grad-accum-steps N    : Gradient accumulation steps (higher = less memory)"
echo "--optimizer TYPE        : Optimizer type (adamw8bit, paged_adamw8bit)"
echo "--zero-stage N          : DeepSpeed ZeRO stage (0=off, 1-3=increasing offload)"
echo "--device-map auto       : Enable HF Accelerate automatic device mapping"
echo "--max-gpu-mem SIZE      : Maximum GPU memory to use (e.g., 39GiB)"
echo "--cpu-mem SIZE          : Maximum CPU memory for offload"
echo "--qlora                 : Enable QLoRA for Transformer blocks"
echo "--batch-size N          : Batch size (smaller = less memory)"

echo -e "\nExpected Results:"
echo "================"
echo "✅ Peak VRAM usage < 39 GiB"
echo "✅ Training completes without OOM"
echo "✅ Model convergence maintained"
echo "✅ Automatic fallback to CPU/disk offload"

echo -e "\nTroubleshooting:"
echo "==============="
echo "If you still get OOM errors:"
echo "1. Increase --grad-accum-steps (try 16, 32)"
echo "2. Decrease --batch-size (try 1)"
echo "3. Use --zero-stage 3 (more aggressive offload)"
echo "4. Enable --qlora for Transformer-heavy models"
echo "5. Check nvidia-smi during training to monitor memory"

echo -e "\nMonitoring Memory:"
echo "=================="
echo "Watch GPU memory during training:"
echo "watch -n 1 nvidia-smi"
echo ""
echo "Or use the built-in memory logging in the training script."